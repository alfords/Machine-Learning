Lecture3
Record by: huangxin

Note:
1.Spark: Hadoop, but not write in disk (write in memory). 
2.For sparse data, need to use small learning rate
3.Each worker sends back the results to the server. The server occasionally sends back the results to the clients. We do not care about whether the parameters the works are now using is up to data or not. (#TODO: explain why it will not affect the results?) 
4.If using adaptive, you cannot use too small learning rate. Otherwise, the changes may be too small. (#TODO: explain why bugs?)
5.Actually, gradient descient is slow. It may be better using () + batch. 
6.

Code analysis:
1. 

